{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\", \"..\"))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from dataset_utils.Dataset import Dataset\n",
    "from dataset_utils.config import DS_ROOT\n",
    "from dataset_utils.utils import imshow_by_path, imshow_by_name, imshow\n",
    "\n",
    "from dataset_utils.Dataset import Dataset\n",
    "from dataset_utils.utils import extract_eyes_region_from_aligned_face, gray, eqhist\n",
    "from dataset_utils.utils import extract_eye_regions_from_aligned_face\n",
    "\n",
    "import data_provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noise detectorの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANT_PATH = \"./flmk_annotation.json\"\n",
    "DATA_DIR = os.path.join(DS_ROOT, \"aligned_with_68\")\n",
    "data = data_provider.get_data(DATA_DIR, ANT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  random.sample(data, len(data))\n",
    "\n",
    "train_d = [d for d in data if d.pid > 3]\n",
    "test_d = [d for d in data if d.pid <= 3]\n",
    "# train_d = data[test_num:]\n",
    "\n",
    "x_train = [d.x for d in train_d]\n",
    "x_test = [d.x for d in test_d]\n",
    "y_train = [d.label for d in train_d]\n",
    "y_test = [d.label for d in test_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension: 408\n",
      "noise num: 727\n",
      "other num: 3100\n",
      "train noise num: 609\n",
      "train other num: 2703\n",
      "test noise num: 118\n",
      "test other num: 397\n"
     ]
    }
   ],
   "source": [
    "print(\"dimension: %d\" % x_train[0].shape[0])\n",
    "\n",
    "print(\"noise num: %d\" % len([d for d in data if d.label == 1]))\n",
    "print(\"other num: %d\" % len([d for d in data if d.label == 0]))\n",
    "\n",
    "print(\"train noise num: %d\" % len([y for y in y_train if y == 1]))\n",
    "print(\"train other num: %d\" % len([y for y in y_train if y == 0]))\n",
    "print(\"test noise num: %d\" % len([y for y in y_test if y == 1]))\n",
    "print(\"test other num: %d\" % len([y for y in y_test if y == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=256)\n",
    "# pca.fit(x_train)\n",
    "# transformed = pca.fit_transform(x_train)\n",
    "# print('各次元の寄与率: {0}'.format(pca.explained_variance_ratio_))\n",
    "# print('累積寄与率: {0}'.format(sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.970713\n",
      "test score: 0.867961\n",
      "f1 score: 0.630435\n",
      "recall: 0.491525\n",
      "precision: 0.878788\n",
      "confusion matrix: \n",
      "[[389   8]\n",
      " [ 60  58]]\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(max_depth=15)\n",
    "# classifier = RandomForestClassifier()\n",
    "classifier.fit(x_train, y_train)\n",
    "print(\"train score: %f\" % classifier.score(x_train, y_train))\n",
    "print(\"test score: %f\" % classifier.score(x_test, y_test))\n",
    "print(\"f1 score: %f\" % f1_score(y_test, classifier.predict(x_test), pos_label=1))\n",
    "print(\"recall: %f\" % recall_score(y_test, classifier.predict(x_test), pos_label=1))\n",
    "print(\"precision: %f\" % precision_score(y_test, classifier.predict(x_test), pos_label=1))\n",
    "print(\"confusion matrix: \\n%s\" % confusion_matrix(y_test, classifier.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noise detectorを利用してデータセットのノイズを検出してjsonにファイル名を書き出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=15, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =  random.sample(data, len(data))\n",
    "data = data_provider.get_data(DATA_DIR, ANT_PATH)\n",
    "X = [d.x for d in data]\n",
    "Y = [d.label for d in data]\n",
    "classifier = RandomForestClassifier(max_depth=15)\n",
    "classifier.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete.\n"
     ]
    }
   ],
   "source": [
    "noise_dict = {}\n",
    "dataset = Dataset(DATA_DIR)\n",
    "data = dataset.data\n",
    "for i, d in enumerate(data):\n",
    "    if d.glasses:\n",
    "        continue\n",
    "    if d.id % 5 != 0:\n",
    "        continue\n",
    "    img = cv2.imread(d.path)\n",
    "    img = eqhist(gray(extract_eyes_region_from_aligned_face(img)))\n",
    "    x = img.flatten()\n",
    "    y = classifier.predict([x])\n",
    "    if y == 1:\n",
    "#         imshow_by_path(d.path)\n",
    "        noise_dict[d.name] = 1\n",
    "print(\"complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DS_ROOT, \"predicted_noise.json\"), \"w\") as fw:\n",
    "    json.dump(noise_dict, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新たに作成した片目ずつのアノテーションファイルからデータセットを構築する\n",
    "- 右目は反転してバリエーションを減らす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANT_PATH = \"./annotations_new.json\"\n",
    "with open(ANT_PATH, \"r\") as fr:\n",
    "    ant = json.load(fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_dir = \"aligned_with_68\"\n",
    "dataset = Dataset(os.path.join(DS_ROOT, \"aligned_with_68\"))\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "\n",
    "class Sample():\n",
    "    def __init__(self, x, img, label, pid):\n",
    "        self.x = x\n",
    "        self.img = img\n",
    "        self.label = label\n",
    "        self.pid = pid\n",
    "        \n",
    "def each_eye_path(path):\n",
    "    ext = path.split(\".\")[1]\n",
    "    left =  path.split(\".\")[0] + \"_left\" + \".\" + ext\n",
    "    right =  path.split(\".\")[0] + \"_right\" + \".\" + ext\n",
    "    return left, right\n",
    "\n",
    "def get_samples():\n",
    "    s = []\n",
    "    for d in dataset.data:\n",
    "        if d.glasses:\n",
    "            continue\n",
    "        if d.id % 5 != 0 or d.id > 50:\n",
    "            continue\n",
    "\n",
    "        lname, rname = each_eye_path(d.name)\n",
    "        img = cv2.imread(d.path)\n",
    "        limg, rimg = extract_eye_regions_from_aligned_face(img, margin=6)\n",
    "        limg, rimg = [eqhist(gray(limg)) for img in [limg, rimg]]\n",
    "        rimg = cv2.flip(rimg, 1) # 右目を左右反転\n",
    "\n",
    "        if lname not in ant:\n",
    "            s.append(Sample(limg.flatten(), limg, 0, d.pid))\n",
    "        elif ant[lname] == \"ok\":\n",
    "            s.append(Sample(limg.flatten(), limg, 0, d.pid))\n",
    "        elif ant[lname] in  {\"big_gap\"}:\n",
    "            s.append(Sample(limg.flatten(), limg, 1, d.pid))\n",
    "\n",
    "        if rname not in ant:\n",
    "            s.append(Sample(rimg.flatten(), rimg, 0, d.pid))\n",
    "        elif ant[rname] == \"ok\":\n",
    "            s.append(Sample(rimg.flatten(), rimg, 0, d.pid))\n",
    "        elif ant[rname] in  {\"big_gap\"}:\n",
    "            s.append(Sample(rimg.flatten(), rimg, 1, d.pid))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos num: 534\n",
      "neg num: 6585\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples()\n",
    "pos = [s for s in samples if s.label == 1]\n",
    "neg = [s for s in samples if s.label == 0]\n",
    "print(\"pos num: %d\" % len(pos))\n",
    "print(\"neg num: %d\" % len(neg))\n",
    "\n",
    "neg = random.sample(neg, len(pos))\n",
    "samples = neg + pos\n",
    "samples = random.sample(samples, len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.997344\n",
      "confusion matrix: \n",
      "[[351   0]\n",
      " [  2 400]]\n",
      "test score: 0.879365\n",
      "f1 score: 0.861314\n",
      "recall: 0.893939\n",
      "precision: 0.830986\n",
      "confusion matrix: \n",
      "[[159  24]\n",
      " [ 14 118]]\n"
     ]
    }
   ],
   "source": [
    "test = [s for s in samples if s.pid < 7]\n",
    "train = [s for s in samples if s.pid >= 7]\n",
    "\n",
    "x_train = [s.x for s in train]\n",
    "y_train  = [s.label for s in train]\n",
    "x_test = [s.x for s in test]\n",
    "y_test  = [s.label for s in test]\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=15)\n",
    "# classifier = GradientBoostingClassifier()\n",
    "# classidier = SVC()\n",
    "classifier.fit(x_train, y_train)\n",
    "print(\"train score: %f\" % classifier.score(x_train, y_train))\n",
    "print(\"confusion matrix: \\n%s\" % confusion_matrix(y_train, classifier.predict(x_train)))\n",
    "print(\"test score: %f\" % classifier.score(x_test, y_test))\n",
    "print(\"f1 score: %f\" % f1_score(y_test, classifier.predict(x_test), pos_label=1))\n",
    "print(\"recall: %f\" % recall_score(y_test, classifier.predict(x_test), pos_label=1))\n",
    "print(\"precision: %f\" % precision_score(y_test, classifier.predict(x_test), pos_label=1))\n",
    "print(\"confusion matrix: \\n%s\" % confusion_matrix(y_test, classifier.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train -> ノイズ検出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.992032\n"
     ]
    }
   ],
   "source": [
    "samples = random.sample(samples, len(samples))\n",
    "X = [s.x for s in samples]\n",
    "Y = [s.label for s in samples]\n",
    "classifier = RandomForestClassifier(max_depth=15)\n",
    "classifier.fit(x_train, y_train)\n",
    "print(\"train score: %f\" % classifier.score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete.\n"
     ]
    }
   ],
   "source": [
    "noise_dict = {}\n",
    "DATA_DIR = os.path.join(DS_ROOT, \"aligned_with_68\")\n",
    "dataset = Dataset(DATA_DIR)\n",
    "data = dataset.data\n",
    "for i, d in enumerate(data[:100]):\n",
    "    if d.glasses:\n",
    "        continue\n",
    "    if d.id % 5 != 0:\n",
    "        continue\n",
    "    img = cv2.imread(d.path)\n",
    "    limg, rimg = extract_eye_regions_from_aligned_face(img, margin=6)\n",
    "    limg, rimg = [eqhist(gray(limg)) for img in [limg, rimg]]\n",
    "    rimg = cv2.flip(rimg, 1) # 右目を左右反転\n",
    "    left = limg.flatten()\n",
    "    right = rimg.flatten()\n",
    "    y_left, y_right = classifier.predict([left]), classifier.predict([right])\n",
    "    if y_left == 1 or y_right == 1:\n",
    "        noise_dict[d.name] = 1\n",
    "\n",
    "print(\"complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3925\n"
     ]
    }
   ],
   "source": [
    "print(len(noise_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DS_ROOT, \"predicted_noise_each_eye.json\"), \"w\") as fw:\n",
    "    json.dump(noise_dict, fw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
